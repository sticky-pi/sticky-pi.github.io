[["index.html", "Sticky Pi, a high-frequency smart insect trap to study daily activity in the field Chapter 1 Introduction", " Sticky Pi, a high-frequency smart insect trap to study daily activity in the field Quentin Geissmann 2022-04-29 Chapter 1 Introduction The principle behind Sticky Pis Sticky Pis are smart sticky traps using a Raspberry Pi camera to automatically score when, which and where insects were captured (using modern AI tools). They take very frequent pictures (three per hour), which reveals much more information compared to traditional traps (such as insects’ response to weather fluctuations and effect of the time of the day). Sticky Pis integrate into a scalable platform, where individual devices send their data to a centralised web-server. A single team can deploy multiple devices and compile large ecological datasets. The Sticky Pi project is a community open-source academic resource. They are affordable and can be easily adapted for research, teaching and applied for work. If you use Sticky Pis in your research, please cite our publication. "],["overview.html", "Chapter 2 Platform Overview", " Chapter 2 Platform Overview The Sticky Pi project contains several interconnected tools. A github organisation features the material (source code, data, CAD files, …) for each individual tool. Here is a list of all the tools: Name Description sticky-pi-device The Hardware and software for the Sticky Pi cameras (i.e. device) sticky-pi-device-v2 The improved version of the smart traps (in development) sticky-pi-data-harvester The Hardware and software for the Data Harvester sticky-pi-data-harvester-v2 The improved version of the Data Harvester (in development) sticky-pi-api Server and client api and the docker services for the server webapps sticky-pi-ml Machine learning for the Sticky Pi project sticky-pi-manuscript Manuscipt material and experiments for the 2021 method publication sticky-pi.github.io Source code of this documentation TODO include image here: platform. A clickable svg would be great This is an overview of the main components of the platform. The details of each part will be explain further. Hardware Sticky Pi devices – Takes pictures of traps and record environmental conditions Data harvester – Manages devices and retrieve their data (in the field) Web server Database – Stores image metadata as well as processing results, users… S3 server – Store and serve image data and machine learning models API – Parses client requests to put/get resources from the underlying database Webapp – A frontend website to visualise/plot the images taken and processing results Nginx server – Secures and routes connections between the above services and the outside world Analysis pipeline – Pre-processes images in real time Machine Learning Universal Insect Detector – Detects insect vs background, in all images Siamese Insect Matcher – Tracks insect instances across imaged Insect Tuboid Classifier – Predicts insect taxonomy from a tracked instance "],["hardware.html", "Chapter 3 Hardware Device Data harvester Using Sticky Pis", " Chapter 3 Hardware This page describes how to build the two main components of the hardware: the Sticky Pi Device and the Data Harvester. Briefly, multiple Sticky Pis are deployed in the field. Each device stores its images on its own SD card. On a regular basis (e.g. weekly), experimenters use a data harvester to retrieve the data from multiple devices. Silently, the data harvester also syncs the location and time of the devices it communicates with. One Data Harvester may be used to maintain multiple traps. Multiple Data Harvesters can exist in the same platform (e.g. five experimenters, one harvester per experimenter, ten devices per experimenter). This section involve familiarity with 3D printing, electronics, DIY, … We are working on streamlining the assembly process towards a more `off-the-shelf tool’, but are generally happy to collaborate and help building devices – do not hesitate to contact us. Device Overview Figure 3.1: Sticky Pi Device As you can see in Figure 3.1, the device has two main parts, which are connected with a ribbon cable: the camera box and the light box. A complete description of the parts, price and reference is available in our Bill of Material Note that all 3D-printed parts are available on our onshape repository where they can be exported in a variety of formats. You can also take a look at our schematic. Assembly The camera Box and light box can be assembled separately and swapped between devices (i.e. the light box acts as a plug and play module for the camera box). Camera box Figure 3.2: Needed parts for the camera box Burn our OS image onto the SD card (note you can also make your own from scratch image by adaptingthe scripts in). Danger Zone! Ensure you know what you are doing here. In particular, give the path to the correct drive otherwise you may wipe out the wrong drive, and lose data. On Unix-like systems, use lsblk to see all drives. To burn an SD card use our script like sh burn_image.sh -i &lt;PATH_TO_LOCAL_IMAGE&gt; -d &lt;DEVICE&gt;, where &lt;PATH_TO_LOCAL_IMAGE&gt; is the OS image you just downloaded, and &lt;DEVICE&gt; the SD card location e.g. /dev/mmcblk0 – you need etcher-cli. Prepare the Raspberry Pi. Figure 3.3: Modifications to the Raspberry Pi Zero Solder 2x5 headers for the Real Time Clock on pins 1-10. Mount the clock (Figure 3.3A). Solder directly the IDC right angle connector onto the board on pins 35-40 (Figure 3.3A). On the back of the Raspberry pi board, use a jumper cable to solder GPIO pin 40 to +5V (this provides power to the Pi, Figure 3.3B) Figure 3.4: Mounting the Raspberry Pi on the sledge Use four screws to mount the pi on the “Pi sledge”. Plug the ribbon cable for the camera (Figure 3.4A). Fold the ribbon cable around the sledge as in the picture, and mount the camera with another four screws (Figure 3.4B-C) Insert the OS SD card (burnt as described above) Focus the camera. For those who want to assemble multiple devices, we recommend a “camera focusing station” that can adapt a Pi sledge and has a display, keyboard… This way, one can use raspivid in preview mode to check focus and alignment. Initialize your device (you can do that at the same time as focusing), which you can do with the Data harvester: Power the data harvester, and ensure the time is set (not it is in GMT/UTC). Plug it to the pi’s USB port. Boot/power the pi. The data harvester should then display information, including the unique ID of the pi. It will be an eight-digit hexadecimal number (e.g. abcd0123). Make a couple of physical label with this tag and stick them to the box (and write them on the sledge). As part of the communication process, the clock of the raspberry pi will be updated (indeed the factory RTC is unset) Thread the 1x6 ribbon cable (it should be at least 50cm long) through the “camera box” before clamping the 2x3 IDC connector on each side. You want have the two connectors in the same orientation (i.e. a wire connect pin 1 of the left connector to pin 1 o the right connector – and not pin 6): Plug the ITC connector on the Pi GPIO, and gently slide the Pi sledge inside the camera box, while pulling the slack cable from the outside. Note, at the stage, you can also add some desiccant inside the box to reduce fogging. Assemble the “camera box lid”: Figure 3.5: Assembly of the camera box lid Cut a glass microscope slide into a 25x25mm square with a glass cutter (e.g. with a 3d printed gauge and snaping tool and tool it takes just a few seconds, Figure 3.5A-C). Place the glass window on the slot, use masking tape or duct tape to cover the central area and the back. Cast a layer of epoxy resin, and let it set for a day this will embed the glass in the lid. Note that it makes sense to make multiple lids at the same time and avoid wasting time and epoxy (Figure 3.5D-E). Clean the lid. Optionally coat it with water repellent such as Rain-X Add calk at the junction between the lid and the box, screw the lid on the box. Also use calk to seal the 6x1 ribbon cable outlet. Coat the whole box with a layer of epoxy (note you can also chose to do that before assembly) Light box Needed parts for the light box Assemble our custom PCB: Figure 3.6: Assembly of the custom PCB Manufacture/order the PCB from the gerber files (Figure 3.6A) Prepare the timer (scratch the board to disable the built-in potentiometer) – (red in Figure 3.6B). Solder all parts to the board. This should be fairly self explanatory from the labels (Figure 3.6C). Make a custom connection to our push button (red in Figure 3.6C). This is arguably a bit of a hack, but that allows us to mimic the built-in TLP5110 push button, with our own. Thread the push button (before soldering it, not to twist the cables) Screw the PCB to the bottom of the light box (be careful not to split the material, do not hesitate to use shorter screw). Figure 3.7: Assembly of the custom PCB Insert the battery connector (Figure 3.7A) Screw the Humidity and temperature sensor to the outside the box (Figure 3.7A-B). Glue the solar panel in the top slot Assemble the light diffuser: Figure 3.8: Assembly of the custom PCB Cut six 5V, white, led light strips (Figure 3.8A). On one side, cut away the + pad, and the - on the other side. Align all the + pads on the top, and solder them together, in parallel, with a transverse wire Solder the 1x2 header to be connected to our PCB as in the picture (the male pins will be protruding downward). Use superglue to glue the header to the diffuser (Figure 3.8B). Test setup: Connect a camera box, using the 1x6 cable. Plug the light diffuser (or just a placeholder 5v LED light for now). Plug a battery. You should see the LED of the timer glowing (if not, press the push button to force reset the timer). After less than a minute, the The flash should be triggered, and the timer should shut down. Pressing the push button should restart the whole process. If left untouched, this should happen every 20min, approximately. QC and troubleshooting Controlling the assembly before deployment in the field will save you a lot of trouble. A few suggestions: Plug all devices, and press the push button to initiate the image acquisition. Ensure all devices flash. The light should blink twice. If the light does not blink. Check power/timer/light connection If the light blinks more than twice, the device is reporting an error. Typically, the camera cannot be found. Plug the Data Harvester and boot a device you want to test. Data transfer should be initiated. Check the label of the pi matches the ID displayed by the harvester. Monitor image transfer. Once all devices have been harvested, retrieve the memory stick of the harvester and inspect the content. There should be one directory for each device Each directory contains all the pictures for this device and a log file Open the log file to check no errors were reported The name of each image contains the time in UTC. Ensure the time is correct. Open the images to check exposure, focus,… The environmental data (temperature, relative humidity, GPS coordinates and other meta variables) are encoded in the exif metadata inside the “Make” field (Sometime, though rarely, the temperature/humidity sensor fails, so you may have missing values). Data harvester The data harvester is a portable tool to retrieve the data from the Sticky Pis. It is a (micro) computer running custom server that are available to the devices through a wifi network (hotspot). Figure 3.9: Sticky Pi Harvester schematic Figure 3.9 show how the different hardware parts interact. The parts we used are described in the Bill of Material. Role of each part: * Raspberry Pi 4 – the core computer * RTC – Real Time Clock: persistent time. * Memory stick – external drive where all the images will be saved. * Mini Router – connect the harvester with the Sticky Pis during data transfer. * Push Button – to turn the Raspberyy Pi off * GPS – optional, get geolocation * Display –optional, show the data harvseter webpage (can be accessed with other devices on the network at 192.168.8.2) Rapsberry Pi Download our OS image and burn it on an SD card Format the USB stick to make a single ext4 partition In the new partition you created, create a file named .api-credentials.env and fill it with: SPI_API_HOSTNAME=&lt;the hostname of the API server&gt; SPI_API_USERNAME=&lt;a username for this data harvester&gt; SPI_API_PASSWORD=&lt;the password for this data harvester&gt; This will allow the data harvester to automatically upload the new images to your sever. This implies you have set up a server, and have created at least one user, which is described the webserver section. Router Set the name of the wifi network to sticky-pi-net, and the password to sticky-pi (this is to match the devices’ environment variables). Ensure the subnet is 192.168.8. (the ip address of the router should then be 192.168.8.1). Write down the MAC address of the Raspberry Pi ethernet port (you can do that by using the ifconfig eth0 command, and spot a six-fields number looking like aa:db:e1:00:01:23). In the DNS menu reserve the IP address 192.168.8.2 to the mac address of the eth0 port you just retrieved Assembly A simple way to make the device is to connect the parts according to the schematics, and arrange them in a junction box like: Figure 3.10: Sticky Pi Harvester Assembled QC and troubleshooting You should be able to see a wifi network named sticky-pi-net and log on to is using the password sticky-pi using any device (phone, table, laptop). Then, when you connect to http://192.168.8.2 a webpage should display the data harvester status. Using Sticky Pis Assuming you have a data harvester and at least one Sticky Pi. You can plug your Sticky Pi, which should begin to take pictures every 20 min. Pushing its button will force it to take a picture and go back to sleep it should take ~40s between when you press the button and when the device shouts the picture (you should see a white flash). If the device turns on and detect a Data Harvester, it will send all the new data automatically. The progress can be monitored on the Harvester webpage. "],["web-server.html", "Chapter 4 Web Server General description Deployment Additional information", " Chapter 4 Web Server This page describes how to deploy and maintain the web server. As much as we try to streamline this process, deploying our web server on the cloud does require some familiarity with Unix/Linux systems, Docker. We are open for collaboration and may be able to help you deploy your server, so do not hesitate to contact us. First, we describe the general function of the server. Second, we explain how to deploy and test it. Last, for completeness, we provide additional information on the individual components. General description Here is a schematics of how these services interact: Figure 4.1: Schematics of the web server services As shown in Figure 4.1, the web server is a suite of docker containers. The entry point is an Nginx server, spi_nginx, routes requests to the appropriate service: spi_api &lt;- api.* – our overall API server (i.e. a server that handles client requests to, for instance, upload/download images, create new users, retrieve image metadata, etc). spi_webapp &lt;- webapp.* – an Rshiny web interface to visualize data in real time. spi_s3 &lt;- s3.* – an optional local s3 server, using localstack. This is an alternative to subscribe to a commercial s3 server. The other services are not routed (i.e must not be accessible by external users): spi_db – a mariadb database that stores image metadata, user data, processing results, … spi_uid – a service that automatically pre-process all images, running the Universal Insect Detector, to segment insects vs background. certbot – a service based on certbot that issues and renews SSL certificates in the background (for HTTPS) Deployment Github repository The all the source code needed for deployment can be found on the sticky-pi-api github repository. It contains two important subdirectories: src, a python package used to create the API, and server, a set of docker services orchestrated with docker compose. For deployment, we only care about the server – if you are interested, the API is documented on readthedoc, but it should only concern you if you want to take part in development. Typically, you would clone this repository on your host. Hosting The simplest way to deploy the Sticky Pi server stack is on a remote Linux virtual machine. Perhaps your institution provides some cloud hosting, for instance Compute Canada Cloud, or you can opt for a commercial cloud provider. You can also decide to run the server on a custom machine within a restricted network. Security A remote web resources implies security considerations: Your host should use firewalls, the web-server only needs ports 80 (http) and 443 (https) open Ensure you use long, high entropy, passwords Update your host system Use ssh keys rather than passwords Configuration and important files Within the server directory, there are a few noticeable files we will describe: deploy.sh – A script you can run to deploy the whole stack. More details later. docker-compose.yml – The base docker-compose configuration file. docker-compose.prod.yml – The specification of the above file, for production. .env – An overall configuration file defining. You will need to modify some variables in it. .secret.env – Contains credentials. For security reasons, this file does not exist yet you need to create it. Do not share this file, and ensure the permissions are restrictive. .env file You need to create a directory that will contain all the server data (except the s3-hosted files). Set LOCAL_VOLUME_ROOT in the .env file accordingly: LOCAL_VOLUME_ROOT=/your/sticky-pi/root/dirctory. In order to encrypt data flow between client and server, we enforce https. That implies the registration of a domain name. Typically, you want to: Ensure your cloud provider can provide a static/floating IP address for your instance, say 111.112.113.114. Find a DNS provider and register your domain name, say sticky.net. Use your DNS provider to create two A records: sticky.net -&gt; 111.112.113.114 and *.sticky.net -&gt; 111.112.113.114 Define ROOT_DOMAIN_NAME in the .env file to your registered domain name. E.g. ROOT_DOMAIN_NAME=sticky.net .secret.env file Create or add to .secret.env file the variables by modifying this template: # En encryption key can be any long sting of ascii characters, # e.g. something like `cfbewif7y8rfy4w3aNIKFW9Yfh89HFN9` SECRET_API_KEY= # Your S3 host. We will describe further the difference between # external and local S3 provider # For local option we can set # S3_HOST=s3.${ROOT_DOMAIN_NAME} # for remote option this would be a hostname like `my-s3.provider.com` S3_HOST= # For local s3 option, you want to define arbitrary keys: # e.g. S3_ACCESS_KEY=fnwfnoAEVikenAV and # S3_PRIVATE_KEY=cfweavb87eabv8uehabv98hwAW7 # For remote option, you will need to issue a key pair and paste it S3_ACCESS_KEY= S3_PRIVATE_KEY= # An arbitrary password, only used internally # Should be more than 10-characters long MYSQL_PASSWORD= # Another arbitrary password, only used internally # Should be more than 10-characters long API_ADMIN_PASSWORD= # Your email address (or the main admin&#39;s) ADMIN_EMAIL= # The name of the S3 bucket storing the images # and other binary files. E.g. `sticky-pi-prod` S3_BUCKET_NAME= # An arbitrary password for the special user/internal bot `uid`, # which can preprocess images automatically (universal insect detector). # Should be more than 10-characters long UID_PASSWORD= Alternative local S3 server Our recommended option is to use an external provider for S3. However, you can also run your own S3 as a docker container. To do that you just need to set S3_HOST=s3.${ROOT_DOMAIN_NAME} in the .secret.env file. Your data will be stored, by default on ${LOCAL_VOLUME_ROOT}/s3. Beware that overall data can be large – typically [1, 2] GB per device per week. Deploy script Once we have set up the environment variables, we are ready to deploy. For convenience, we provide a script: server/deploy.sh that should be used this way for a production server: # sh deploy.sh prod-init # sh deploy.sh prod prod-init initializes SSL certificates. You should need to do that only once. If you want to subsequently restart/redeploy the server, just run sh deploy.sh prod. The first time, it may take a wile to build all the docker images. Once all the images have been built, ensure containers are running using docker ps. you can also debug and check the logs by using docker logs &lt;service_name&gt;. Check the API The simplest way to test the API end to end is to use cURL – you could also use our python client. For instance, we can ask the API to issue a temporary logging token for the admin user. Replace &lt;API_ADMIN_PASSWORD&gt; and &lt;ROOT_DOMAIN_NAME&gt; by their value (same as in .secret.env). curl --user admin:&lt;API_ADMIN_PASSWORD&gt; -X POST https://api.&lt;ROOT_DOMAIN_NAME&gt;/get_token This should return a json dictionary with the fields &quot;expiration&quot; and &quot;token&quot;. If this does not work, stop here, and debug. Create users We can also use cURL to create users. For instance, to create a read only user named spi, with password R3ad_0nly, and no email address, we can run: curl --user admin:&lt;API_ADMIN_PASSWORD&gt; -X POST -H &quot;Content-Type: application/json&quot; -d &#39;[{&quot;username&quot;: &quot;spi&quot;, &quot;password&quot;: &quot;R3ad_0nly&quot;, &quot;is_admin&quot;: 0, &quot;email&quot;: &quot;&quot;, &quot;can_write&quot;:0}]&#39; https://api.&lt;ROOT_DOMAIN_NAME&gt;/put_users &quot;can_write&quot;:1 would create a user that can write, and &quot;is_admin&quot;: 1, an admin user (e.g. that can create other users). Note that you will not be able to retrieve a user’s password as passwords are internally encrypted. If a password is lost, the only way to log in is to reset it. You can list users with: curl --user admin:&lt;API_ADMIN_PASSWORD&gt; -X POST https://api.sticky-pi.com/get_users You can test users by requesting a token, using curl as above, replacing --user admin:&lt;API_ADMIN_PASSWORD&gt;, by --user &lt;USERNAME&gt;:&lt;USER_PASSWORD&gt;. Backups The raw image data is on the s3 server. Manual backups/archives can be done by downlaoding all data to a separate resource/hard drive/… with tools such as s3cmd, using your credentials. In order to backup mariadb, you can use the deploy script: #sh deploy.sh mysql-backup This will create a file like spi_db.dump.YYYY-MM-DD.sql.gz in the current working directory – that you then manually copy/archive. In case of catastrofic failure, the database could be restored with it. Additional information Database The database (spi_db) is the stock MariaDB. The database data is mounted on a persistent directory on the host: ${LOCAL_VOLUME_ROOT}/mysql in the host maps /var/lib/mysql in the container. Nginx Nginx (spi_nginx) is the stock Nginx. We use a custom configuration file to define routing (nginx/nging.conf-template). Note that this is a template file where we inject environment variable at built time. You should not need to modify the configuration file for standard deployment API The API (spi_api) is a uwsgi + flask server. The main file is api/app.py. This server depends extensively on our python package, sticky_pi_api. Note that the API and client are packaged together, which allows us to use mock local API vs remote API. Universal Insect Detector The UID (spi_uid) is a simple python script (ml/uid.py) that uses the API client and the our sticky_pi_ml package. It fetches the images that are not annotated (or obsolete) from client, analyse them, and upload the results (to the database, through the API). This process runs automatically. Note that you need to have either trained the UID or download a reference model for this service to work. We explain how to set-up ML resources in a dedicated section. Webapp The Webapp (spi_webapp) is an Rshiny interactive website. It is mainly designed to visualise image and processing data. Typically hundreds of images are acquired every week, this interface allows you to select a date range to display a plot of environmental conditions and overlay images as well as UID results. This is very helpful to ensure the quality of your data in real time. The webapp will be available at webapp.&lt;ROOT_DOMAIN_NAME&gt;, as defined in hosting. "],["ml.html", "Chapter 5 Machine Learning General description General Prerequisites Universal Insect Detector Siamese Insect Matcher Insect Tuboid Classifier Conclusion", " Chapter 5 Machine Learning This page describes how to annotate, train and use the machine learning model of the Sticky Pi project. This step does require some familiarity with scientific computing (e.g. Unix/Linux, python. Here, we complement the descriptions of the algorithms provided in the Sticky Pi manuscript with a practical documentation on how to use and adapt them. We are open for collaboration regarding training, building and extending Machine Learning models, so do not hesitate to contact us. For simplicity, we describe the how to use the ML tools independently of the API (which is set to store and query images/annotations on a remote or a local machine). For production, with multiple concurrent Sticky Pi devices, we recommend using the API. General description Briefly, we start from a series of images (every 20min, for several day) of a sticky card. Our ultimate goal it to tell which, and when insects were captured. We break down this task in three independent steps: Universal Insect Detector – We perform an instance segmentation on all images independently. This extracts insects (foreground) from the background. Importantly, at this stage, we do not yet classify insects. Siamese Insect Matcher – Captured insect actually may move, degrade, become occluded… Therefore, in practice, classification and timing of capture from single images would be very inconsistent through time. Instead, we first track insects before classifying them. The function of the Siamese Insect Matcher is to track multiple insect instances through their respective timelapses. Insect Tuboid Classifier – After tracking, each insect in the time series is represented by a variable number of standardized segmented shots as well as metadata (including size) – which we call a “tuboid”. The Insect Tuboid Classifier infers a taxonomy to each tuboid based on multiple images. Below, we describe how to implement each step. The model files and datasets used in the publication are available on Zenodo. Our source code is publicly available on github. General Prerequisites Installation We recommend starting by setting a Python virtual environment for the entire project. And using the python package manager (pip). Detectron2, PyTorch and PyTorch Vision In your virtual environment, you want to manually install detectron2, which requires matching PyTorch and Torchvision. This has to be done manually since it depends on your platform/hardware (e.g. GPU support). For instance, on a Linux machine without CUDA (so, no GPU support), from the pytorch website: # installing precompiled PyTorch and Torchvision (from at https://pytorch.org/) pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu # then building detectron2 from source # you could also use prebuilt versions pip3 install &#39;git+https://github.com/facebookresearch/detectron2.git&#39; Note, to make the most of CNNs and PyTorch, you likely want to have hardware support (i.e. a GPU). Running models on a CPU is mostly for testing and development, and may be very slow (in particular for training). sticky-pi-ml Once you have installed detectron2 (along with PyTorch and PyTorch Vision), you can install our package and its dependencies. git clone https://github.com/sticky-pi/sticky-pi-ml.git --depth=1 cd sticky-pi-ml/src pip install . Project organisation In the Sticky Pi project, the resources for each of the three algorithms described above (i.e. UID, SIM and ITC) are stored and organised in a “Machine Learning (ML) bundle”. An ML bundle is a directory that contains everything needed to train, validate and use a tool. ML bundles all contain the subdirectories: config – one or several .yaml configuration files data – the training and validation data output – the trained model (i.e. .pth files). The file model_final.pth being the working model used for inference. Universal Insect Detector The goal of (Universal Insect Detector) UID is to find and segment all individual insects from arbitrary sticky card images. As part of the sticky-pi-ml package, we have made a standalone tool version of the UID, standalone_uid.py (see standalone_uid.py --help). From within your virtual environment, you can use this tool (it should be in your path after installation) to segment images as well as re-train and validate the model on your own data. We can start by downloading the whole ML Bundle from Zenodo (that can take a while as we are getting both the model and the data): wget https://zenodo.org/record/6382496/files/universal-insect-detector.zip unzip universal-insect-detector.zip # show what is inside this new directory ls universal-insect-detector Our bundle directory is therefore universal-insect-detector. Inference For inference with the standalone tool, all images should be .jpg stored in a directory structure. For this example, you could download a sample of three images we have put together for this tutorial. wget https://doc.sticky-pi.com/assets/uid_tutorial.zip unzip uid_tutorial.zip ls uid_tutorial Or just use the download link Then, we can use our custom script standalone_uid.py to “predict” – i.e. segment the images: # We use the uid tool to predict, based on a model located in the bundle `./universal-insect-detector` (--bundle-dir) # We find all `.jpg` files in the target directory `./uid_tutorial` (--target) # We set the verbose on (-v) standalone_uid.py predict_dir --bundle-dir ./universal-insect-detector \\ --target ./uid_tutorial -v # list the generated resulting files ls ./uid_tutorial/*.svg As a result, the uid tool makes an SVG image for each JPG. The SVG contains a path for each detected instance. You can directly open the SVG files to check. By default, the inference tool does not overwrite existing SVG, unless you use the --force flag. Training Data source In order to train the model, you need to populate the universal-insect-detector/data/. For the UID, the input files are SVGs exactly like the ones outputted by the inference tool (i.e. a JPG image is embedded and each insect is a path). You can either add new data to the existing collection of SVGs already present in universal-insect-detector/data, or rebuild your own new set (though the later option would be rather labour-intensive). A simple way to get started, is to run inference on your new images (as describe just above), and fix/check the resulting SVGs by hand. Alternatively, you use the --de-novo option along with --predict-dir to just wrap your images in an empty SVG. To edit the SVG, we recommend using inkscape. The default is that every insect is a simple, closed, path/polygon (e.g. made with the Bezier curve tool). The stroke colour of the path defines the class of the object (the filling colour does not matter). The default stroke colour for insects is in blue #0000FF (This is defined in the configuration file. Any other colour will not be recognised as an insect): UID annotations. Using inkscape to generate annotations as SVG paths Configuration There are two configuration files for the UID: mask_rcnn_R_101_C4_3x.yaml – the configuration for Mask-RCNN as defined in the detectron2 documentation. This is the underlying segmentation algorithm we use. config.yaml – The specific configuration for the Sticky Pi project (which may override some of the mask_rcnn_R_101_C4_3x.yaml configuration). See inline comments for detail. The important configuration variables are likely going to be in the SOLVER section: IMS_PER_BATCH – the number of images in a training bash. The larger this number, the more memory will be used during training. This will depend on your GPU capabilities. BASE_LR – The starting learning rate GAMMA – The decay rate of the learning rate, at every ‘step’ STEPS– The training steps, in number of iterations. at each step, the learning rate will decrease (by a factor GAMMA) Data integrity Before training, you most likely want to check your data can indeed be loaded and parsed. standalone_uid.py check_data --bundle-dir ./universal-insect-detector -v Read carefully the warnings, in particular if they hint that the SVG paths are malformed. Training The training itself can be very long (e.g. several days on a GPU can be expected). Likely, you have access to specialised hardware and support to do that. Once you have set the configuration, checked the input data, etc, you can use the standalone UID tool to make a new model. standalone_uid.py train --bundle-dir ./universal-insect-detector -v The script will output information about the dataset and print a summary every 20 iterations (by default). Each summary contains information such as total_loss, which should eventually decrease (this is described in the detectron2 documentation). Every 5000 iteration (defined in the configuration as SOLVER/CHECKPOINT_PERIOD) a snapshot of the ongoing model will be generated as universal-insect-detector/output/model_XXXXXX.pth, where XXXXXX is the iteration number. Unless you have reached the maximal number of iteration, you will need to manually copy your latest snapshot into the final working model universal-insect-detector/output/model_final.pth. You could use the intermediary snapshots to perform custom validation, or eventually remove them. If you want to train the model from “scratch”, use the --restart_training flag. This will actually use Mask-RCNN model that was pretrained on the COCO dataset (training from zero would be much longer). Validation An important step is validation. By default, each original image is allocated to either a validation (25%) or training (75%) dataset. This is based on the checksum of the JPG image, so it is pseudorandom. To compute validation statistics on 25% of the images that were “randomly” excluded from training, you can run: standalone_uid.py validate --bundle-dir ./universal-insect-detector \\ --target validation_results -v This will run an inference on the validation set which has not been seen during training, create a resulting SVG files and issue summary statistics for each validation image (all in the target directory validation_results/). In particular, there will be a resulting JSON file results.json, which contains a list where each element is a detected instance. Each instance has the fields: area – the number of pixels in the instance in_gt – whether the instance is in the ground truth in_im – whether the instance is in the detected image class – the class (i.e. insect) filename – the SVG file where the image is from You can then parse this file (e.g. in R) to compute summary statistics: library(data.table) library(jsonlite) dt &lt;- as.data.table(jsonlite::fromJSON(&#39;results.json&#39;)) dt[, .(precision = sum(in_gt &amp; in_im)/ sum(in_im), recall = sum(in_gt &amp; in_im)/ sum(in_gt)), ] You can also compare the validation images (SVG) generated in the target directory (validation_results/). Each path is a detected instance. The ones filled with red (#ff0000) are detected by the UID, whilst the blue (#0000ff) ones are the ground truth. I find it convenient to open images in inkscape and select, say a UID-generated path, right click and press “select same/fill and stroke” to then change the stroke style and colour to visualise better: Validation results. Left, the original svg; Right, highlighting the ground truth in thick green strokes Siamese Insect Matcher The Siamese Insect Matcher is the second step of the analysis. We start from a series of images annotated by the UID to generate “tuboids”, which are series of shots of the same insect, over time. To do that, we use annotated Sticky Pi images. In the previous section, we described how to generate SVG images from JPGs in a directory. The SIM is specific to Sticky Pi images as their timestamp is encoded in their name. The name of each image is formatted as &lt;device_name&gt;.&lt;datetime&gt;.jpg. We also have a standalone tool to use the SIM standalone_sim.py (see standalone_sim.py --help). We can start by downloading the whole ML Bundle from Zenodo (that can take a while as we are getting both the model and the data): wget https://zenodo.org/record/6382496/files/siamese-insect-matcher.zip unzip siamese-insect-matcher.zip # show what is inside this new directory ls siamese-insect-matcher Inference For this documentation, we have made available a small series of 50 pre-annotated images. First we download the images: wget https://doc.sticky-pi.com/assets/sim_tutorial.zip unzip sim_tutorial.zip ls sim_tutorial Or just use the download link. Using the standalone tool, you can then generate tuboids: standalone_sim.py predict_dir --bundle-dir ./siamese-insect-matcher \\ --target ./sim_tutorial -v This should show progress and processing information. As a result, you will find a directory named tuboids in --target (i.e. ./sim_tutorial), with the following structure: tuboids/ └── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f ├── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f.0000 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg ├── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f.0001 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg ..................... SKIPPING DIRECTORY WITH SIMILAR STRUCTURE ..................... ├── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f.0019 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg └── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.mp4 The parent directory, 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f, is formatted as: &lt;device_id&gt;.&lt;start_datetime&gt;.&lt;end-datetime&gt;.&lt;algorithm_unique_id&gt;. The children directories are unique identifier of each tuboid in this specific series (ending in .0000, .0001, .0002, …). Inside each tuboid directory, there are three files: metadata.txt – a small file describing each insect (parent image, x and y position, and scale) tuboid.jpg – a JPG file where shots listed in metadata are represented as contiguous tiles. The file is padded with empty space when necessary context.jpg – a compressed version of the first image containing the insect, where the insect is boxed (to show where and how large it is). This is mostly to give some context to the annotating team. You will also find a video (15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.mp4): Note that each tuboid is shown by a rectangle with a number matching the unique identifier of a tuboid. This video can help you find a specific tuboid in the tuboids directory structure. For instance, tuboid number 12 looks like this: Also, the UTC date and time are written on top of the video. Training The core algorithm of the Siamese Insect Matcher is a Siamese neural network. It compares insects from consecutive picture and learns to discriminate between the same instance vs. another insect. The data to train this network is a set of paired UID-annotated images, where insects that are the same instances are labeled as a group. In practice, these are encoded as composite SVG images with the two images vertically stacked, and the instances labeled as an SVG group. Rather than manually generating a training set, we can use the standalone tool to generate candidates for us that we can then just amend: standalone_sim.py make_candidates --filter 5 --target ./sim_tutorial -v The argument --filter 5 keep only every fifth image, and skip the others. By default, this process will pre-group pairs of contours that highly overlap between consecutive images (based on their IoU only). This should save a lot of time as most of these should be simple cases. This tool also draws a line to highlight paired groups. The line is simply a visual tool to help annotation and can be left as is: Generate SIM annotations in Inkscape Using inkscape, you can group pairs using a shortcut like CTRL+G. We recommend using the XML editor (the panel on the right) to check grouping is correct, and find unmatched instances. Keep in mind that not all instances can / should be matched. Indeed, some insects may be mislabeled in a first place, or may have escaped, appeared, … It is unnecessary (and probably counterproductive) to try to manually fix the segmentation (i.e. the UID) by adding or deleting new instance annotations (paths). As before, you can either make your own data set, or just extend the existing SIM data bundle (the latter is recommended). For the training itself, we can run: standalone_sim.py train --bundle-dir ./siamese-insect-matcher -v On a machine with CUDA, you should use the --gpu flag. Also you can use the --restart-training flag to restart from scratch rather than use the previous weights. The training involves two preliminary stages that independently train separate branches of the network followed by a longer third (final) stage that trains the whole network. Every 200 rounds (default), the model takes a snapshot (i.e. saves the weights) in siamese-insect-matcher/output/model_XXXXXX.pth. You can also manually the any .pth file as model_final.pth to specifically use this model during inference Validation Like for the UID, each original image is allocated to either a validation (25%) or a training (75%) dataset based on a checksum. Validation is automatically computed to generate a loss and accuracy when training reaches a checkpoint (by default, every 200 rounds). One can also specifically run validation: standalone_sim.py validate --bundle-dir ./siamese-insect-matcher \\ --target ./sim_tutorial/validation/ -v This will generate a json file ./sim_tutorial/validation/results.json, which is a list of dictionaries with fields pred (predicted score) and gt (ground truth, i.e. 0 or 1). pred is the raw match score (between 0 and 1), so one can use this result file to vary the match threshold and make an ROC curve. E.g. to make a ROC curve in R: library(plotROC) library(data.table) library(jsonlite) dt &lt;- as.data.table(jsonlite::fromJSON(&#39;results.json&#39;)) rocplot &lt;- ggplot(dt, aes(m = pred, d = gt))+ geom_roc(n.cuts=20,labels=FALSE) rocplot + style_roc(theme = theme_grey) + geom_rocci(fill=&quot;pink&quot;) Insect Tuboid Classifier The Insect Tuboid Classifier (ITC) is the last step of the analysis. Its goal is to infer taxonomy from tuboids (several shot of an insect). It is an adapted ResNet architecture that: takes multiple images as input, average them on the feature space, and outputs one prediction. This algorithm also uses the initial pixel-size of the insect, which is contained in the tuboid metadata. As opposed the other two generalist algorithms, the ITC will need to be retrained for specific contexts (location, season, …). Therefore, we will first describe how to train the algorithm. Training Ground-truth data The previous algorithm (SIM) generated “tuboids”, each of which corresponds to a unique insect. These are uniquely identified directories. The goal of the annotation is to associate a taxonomy to a set of reference tuboids. In order to facilitate annotation of many tuboid, we have made an open source multi-user web tool. Taxonomy is the hardest task, so multiple experts might be involved. In order to use the annotation tool, two steps are required: Set up an S3 bucket that contains the tuboid data. Configure and deploy our docker service Upload data on an S3 server S3 servers are a standard cloud solution for hosting and serving large amounts of data on the cloud. There are different providers and tools to interface S3 server. Here, we show how to do that with s3cmd. First, we make a new bucket: S3_BUCKET=&quot;a-sensible-name&quot; s3cmd mb s3://${S3_BUCKET} Then, we upload the local tuboid data. We assume, you have a directory ($TUBOID_DATA_DIR) with the tuboid data. This would be typically a subdirectory named tuboids in the --target directory you set when using the SIM for inference. To practice, you can use tutorial data we have put together for the ITC: wget https://doc.sticky-pi.com/assets/itc_tutorial.zip unzip itc_tutorial.zip ls itc_tutorial Or just use the download link TUBOID_DATA_DIR=itc_tutorial/data # or REPLACE_WITH_YOUR_PATH # this send the local data on the remote s3 server s3cmd sync ${TUBOID_DATA_DIR}/ s3://${S3_BUCKET} Obviously, check that files are being uploaded. We also pre-generate an index file that list all the tuboids. TMP_DIR=$(mktemp -d) echo &#39;tuboid_id, tuboid_dir&#39; &gt; ${TMP_DIR}/index.csv # this loop generates the index file for i in $( s3cmd ls s3://${S3_BUCKET}/ --recursive | \\ grep metadata\\.txt | \\ cut -d : -f 3 | \\ cut -c2-); do echo $(basename $(dirname $i)), $(dirname $(realpath --relative-to=&quot;/${S3_BUCKET}&quot; $i -m)) \\ &gt;&gt; ${TMP_DIR}/index.csv; done # for each tuboid, 3 files are uploaded (`metadata.txt`, `tuboid.jpg` and `context.jpg`) head ${TMP_DIR}/index.csv # just to take a look at the index file wc -l ${TMP_DIR}/index.csv # the number of line should be the number of tuboids +1 (for the header) s3cmd put ${TMP_DIR}/index.csv s3://${S3_BUCKET} &amp;&amp; rm ${TMP_DIR} -r On your s3 service provider, generate an access key specifically for the docker service. Ideally, the access key pair is read only and only has access to the bucket you just created. Do not share the private key, but keep it as you will need it to configure the docker service. Set up the docker service Docker is an industry standard container service. It allows you to run a virtualised self-contained service on top of a host machine. You can set docker on your machine, a local or a remote server. There are also a range of commercial docker hosting platform. To run our docker container, first create a local environment file to store your variables, including s3 credentials, for instance .secret.env (ensure this file) has the appropriate access set (i.e. only you/ the administrator should be able to read this file). In .secret.env, we can edit the environment variables according to the comment: export S3_HOST=some.host.com # The host of the bucket e.g. `location. export S3_BUCKET=my-bucket-name # the name of the s3 bucket. ame as above export S3_PRIVATE_KEY=abcdefgh # The private key for this service (see above) export S3_ACCESS_KEY=ABCDEF # The access key for this service (see above) export PORT=80 # The network port on the host on which the service will be available (80 for http) export LOCAL_VOLUME=/my/local/storage # A local dirctory on the host that contain the service&#39;s data (e.g. the results) Then to run the container, you can do: source ./.secret.env docker run --rm --publish ${PORT}:80 \\ --name tuboid-annotation-tool \\ --env S3_BUCKET=${S3_BUCKET} \\ --env S3_HOST=${S3_HOST} \\ --env S3_ACCESS_KEY=${S3_ACCESS_KEY} \\ --env S3_PRIVATE_KEY=${S3_PRIVATE_KEY} \\ --volume ${LOCAL_VOLUME}:/opt/data_root_dir \\ -d stickypi/tuboid-annotation-tool:latest The first time, this will download the image from the dockerhub, where we have stored a copy of the prebuilt docker image. For this simple application, we just use plaintext credentials as a json file, credentials.json, in LOCAL_VOLUME. For instance, in ${LOCAL_VOLUME}/credentials.json, yo0u could have: { &quot;user1&quot; : {&quot;password&quot;: &quot;password1&quot;, &quot;allow_write&quot;: 1}, &quot;user2&quot; : {&quot;password&quot;: &quot;password2&quot;, &quot;allow_write&quot;: 0} } This creates two users, user1 and user2, with passwords, password1 and password2, respectively. allow_write=1 means the user (user1) has write access. user2 can only view images, but cannot annotate. If you run the annotation tool on your local computer, you should be able to access it on http://localhost:&lt;PORT&gt;, where &lt;PORT&gt; is the port you defined in your configuration file. This video shows how you can use the tool to annotate tuboid images and export the data: Training the algorithm Once you have annotated your data, you can retrieve database.db file from the annotation web tool. You will need to put this file in the data subdirectory of your bundle directory. If you have used the tutorial bundle, it should look like that: ├── config │   └── config.yaml └── data ├── database.db ├── 0a5bb6f4.2020-07-08_22-00-00.2020-07-15_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694 │   └── 0a5bb6f4.2020-07-08_22-00-00.2020-07-15_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694.0000 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg ├── 15e612cd.2020-07-01_22-00-00.2020-07-08_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694 │   ├── 15e612cd.2020-07-01_22-00-00.2020-07-08_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694.0000 │   │   ├── context.jpg │   │   ├── metadata.txt │   │   └── tuboid.jpg ..................... SKIPPING DIRECTORY WITH SIMILAR STRUCTURE ..................... As for the two other tool, we have built a standalone executable for the ITC. Given itc_tutorial/ is your bundle dir, you can then train the model using: standalone_itc.py train --bundle-dir itc_tutorial/ This will start training the model from scratch and output validation statistics and confusion matrices at every checkpoint. As a functional example, you can download and train the model we used for the manuscript: We can start by downloading the whole ML Bundle from Zenodo (that can take a while as we are getting both the model and the data): wget https://zenodo.org/record/6382496/files/insect-tuboid-classifier.zip unzip insect-tuboid-classifier.zip # show what is inside this new directory ls insect-tuboid-classifier Importantly, you need to adapt the configuration file config.yaml to your scenario. In particular, in this model, we define flat labels from taxonomy, using regular expressions. The taxonomical levels are: type, order,family,genus,species, and extra (extra could be other information such as sex and morph). Labels are defined as [regex, number], in config.yaml: LABELS: - [&#39;^Background.*&#39;,0] - [&#39;^Insecta\\.Hemiptera\\.Cicadellidae\\.Edwardsiana.*&#39;, null] - [&#39;^Insecta\\.Hemiptera\\.Cicadellidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Drosophilidae\\.Drosophila\\.Drosophila suzukii.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Drosophilidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Psychodidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Culicidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Muscidae.*&#39;,null] - [&#39;^Insecta\\.Diptera\\.Sciaridae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Syrphidae.*&#39;, null] - [&#39;^Insecta\\.Coleoptera\\.Curculionidae.*&#39;,null] - [&#39;^Insecta\\.Coleoptera\\.Coccinellidae.*&#39;,null] - [&#39;^Insecta\\.Coleoptera\\.Elateridae.*&#39;,null] - [&#39;^Insecta\\.Coleoptera.*&#39;,null] - [&#39;^Insecta\\.Hymenoptera\\.Figitidae.*&#39;, null] - [&#39;^Insecta\\.Hymenoptera\\.Halictidae.*&#39;,null] - [&#39;^Insecta\\.Lepidoptera.*&#39;, null] - [&#39;^Insecta.*&#39;,1] regex is a regular expression matching the taxonomy. All taxonomical levels are collapsed in a single sting separated by .. For instance, Drosophila suzukii is defined as ^Insecta\\.Diptera\\.Drosophilidae\\.Drosophila\\.Drosophila suzukii.* number is an integer &gt;=0, for this label. If null, the labels take automatically the next available value. (here we set background and undefined insects as 0 and 1, respectively). Since multiple labels may match an annotation, the order of definition of labels matter. A given annotation will take the value of the first matched label. For instance, if we have an annotation that is Insecta\\.Coleoptera\\.Elateridae.abcd.efg, it matches both ^Insecta\\.Coleoptera\\.Elateridae.* and ^Insecta\\.Coleoptera.*. Since ^Insecta\\.Coleoptera\\.Elateridae.*, comes first, we keep it as a label. Inference Since we already have a functional model for the manuscript data, we can use it to demonstrate how the inference works. Given you store the ML bundle for the ITC in insect-tuboid-classifier/ we could run inference, using this model, on its own data --target insect-tuboid-classifier/data. standalone_itc.py predict_dir --bundle-dir insect-tuboid-classifier --target insect-tuboid-classifier/data -v This will generate a CSV file, insect-tuboid-classifier/data/results.csv&quot; with column names describing the taxonomy and the name/origin of unique tuboids in the column directory. Conclusion We have covered the basics of how to perform machine learning with the Sticky Pi project. Hopefully this is a good starting point to use and adapt our tools. The project is still active, and we are always welcoming contributions and collaborations. "],["outreach.html", "Chapter 6 Outreach UBC Campus Living Lab June-August 2021", " Chapter 6 Outreach Sticky Pis are built on top of community hardware such as Arduino and Raspberry Pi as well as open-source software such as Python. We share their educational spirit and consider it our work to raise awareness about insect ecology and discuss how new technologies can indeed intersect with the study of insects. UBC Campus Living Lab In 2022, we were awarded the Campus Living Lab fund by UBC sustainability. The CLL will allow us to improve and deploy Sticky Pis all over the UBC campus to monitor biodiversity in real-time. We will also involve the community by running workshops and hackathons. June-August 2021 Figure 6.1: Sticky Pi at the UBC Research Farm In 2021, we were able to deploy 36 Sticky Pi traps at the University of British Columbia farm, a research, teaching and learning space, located on the traditional, ancestral, and unceded territory of the hən̓q̓əmin̓əm̓-speaking xʷməθkʷəy̓əm (Musqueam) people, and housed under the Faculty of Land and Food Systems. Our goal was to monitor insect populations in an organic strawberry field interspersed with different flowers (aka intercropping). Neighbouring flowers are known to attract pollinators and other beneficial insects that can then help to control pests. We wanted to study how different flowers such as buckwheat, clover and dill indirectly mitigate the infestation by the spotted wing drosophila (a voracious fruit pest). Intercropping could be a tool within a suite of integrated solutions to maintain production whilst reducing the environmental impact of agriculture. The experiment is ongoing and we will communicate the results later this year. "],["community.html", "Chapter 7 Issues and community Citation Having troubles Contributing Contact", " Chapter 7 Issues and community Citation Sticky Pis are a research tool. As such, they are – and will always be – as free and open-source as possible. If you use Sticky Pis as part of your research, please cite our reference publication. Having troubles If you are having issues or you want help with something, the best thing you can do is fill an “issue” on the github repository of the relevant tool. Contributing We welcome external contributions and hope Sticky Pis develops as part of a computational entomology community. There are several ways you can contribute: By requesting features and reporting bugs through the github issue system By sending pull requests to preexisting tools By volunteering to maintain/develop a tools Contact Sticky Pis are developed at the Plant-Insect Ecology and Evolution lab at UBC, Vancouver in collaboration with the Haney Lab and the Insect Biocontrol lab. Do not hesitate to contact Quentin Geissmann, the main contributor of the project for inquiries. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
