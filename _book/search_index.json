[["index.html", "Sticky Pi, a high-frequency smart insect trap to study daily activity in the field Chapter 1 Introduction", " Sticky Pi, a high-frequency smart insect trap to study daily activity in the field Quentin Geissmann 2023-04-26 Chapter 1 Introduction The principle behind Sticky Pis Sticky Pis are smart sticky traps using a Raspberry Pi camera to automatically score when, which and where insects were captured (using modern AI tools). They take very frequent pictures (three per hour), which reveals much more information compared to traditional traps (such as insects’ response to weather fluctuations and effect of the time of the day). Sticky Pis integrate into a scalable platform, where individual devices send their data to a centralised web-server. A single team can deploy multiple devices and compile large ecological datasets. The Sticky Pi project is a community open-source academic resource. They are affordable and can be easily adapted for research, teaching and applied for work. If you use Sticky Pis in your research, please cite our publication. "],["overview.html", "Chapter 2 Platform Overview", " Chapter 2 Platform Overview The Sticky Pi project contains several interconnected tools. A github organisation features the material (source code, data, CAD files, …) for each individual tool. Here is a list of all the tools: Name Description sticky-pi-device The Hardware and software for the Sticky Pi cameras (i.e. device) sticky-pi-android-harvester An android app to harvest the data and interact with the sticky pis sticky-pi-api Server and client api and the docker services for the server webapps sticky-pi-ml Machine learning for the Sticky Pi project sticky-pi-manuscript Manuscipt material and experiments for the 2021 method publication sticky-pi.github.io Source code of this documentation This is an overview of the main components of the platform. The details of each part will be explain further. Hardware Sticky Pi devices – Takes pictures of traps and record environmental conditions Data harvester – Manages devices and retrieve their data (in the field) Web server Database – Stores image metadata as well as processing results, users… S3 server – Store and serve image data and machine learning models API – Parses client requests to put/get resources from the underlying database Webapp – A frontend website to visualise/plot the images taken and processing results Nginx server – Secures and routes connections between the above services and the outside world Analysis pipeline – Pre-processes images in real time Machine Learning Universal Insect Detector – Detects insect vs background, in all images Siamese Insect Matcher – Tracks insect instances across imaged Insect Tuboid Classifier – Predicts insect taxonomy from a tracked instance "],["hardware.html", "Chapter 3 Hardware Overview Assembly graph Modules", " Chapter 3 Hardware This page describes how to build a Sticky Pi. It involves familiarity with 3D printing, electronics, DIY, … First, we present an short overview of the device itself. Second, we describe the “assembly graph” to construct it from basic parts. Third, we present the minimal attachments/modules needed to test abd operate it (battery, sticky card holder). We are working on streamlining the assembly process towards a more ‘off-the-shelf tool’, but are generally happy to collaborate and help building devices – do not hesitate to contact us. Overview As shown here, Sticky Pis have two main part, the camera box, which contains all the electronic and the camera, and the lightbox, which works like a backlight when images are taken. These two parts are connected by metal rods. Figure 3.1: Assembled and deployed Sticky Pi, version 2 The complete list of parts and consumables required (i.e. Bill Of Material) is available here. Custom Board The first part is a custom board, the Sticky Pi Hat, that we designed to simplify the assembly process. This board has its own BOM. The KiKad project (including schematics) and Gerber files are available here. The Sticky Pi Hat and can be readily machine-assembled in a factory. We are happy to assist users with ordering. 3D-printed parts All 3d printed parts are available on our Onshape repository. Snapshots of individual stl files are also on our github here Assembly graph We decided to describe the assembly process as an interactive graph. The assembly process goes from the top down. Each part is a rectangle on which you can click to access its documentation. Each diamond shape, is a “process” on which you can click to show a video and written description. You can also use the search bar to find parts or processes. Here is an example showing how to use this interactive documentation. Interactive building instructions Navigate and click on the graph to display instructions Link. Estimate (per device) = $ Modules In addition to the Sticky Pi itself, in most cases, you will need a power module (e.g. a battery), and a support to hold sticky cards. Here, we describe these two essentials, as well as the station we designed to test devices. Battery module Sticky Pis can accommodate various power solutions – as long as you provide 3.5-6V continuous current, and at least 500mA. Power needs to be provided through a female 2-pin auto connector (that matched the sticky Pi male connector), often you would just buy them in pairs. What battery you pick depends on your use case, In some cases you can also adapt a USB cable to provide 5V to sticky pis without a battery (e.g. in a lab or a green house). The top of the lightbox exposes a flexible attachment we intended for batteries. In most of our use cases, we opted for off-the shelf tablet lithium ion batteries such as this one. Then we encased the batteries battery in epoxy with custom pegboard pins to hang the it on the light box: Figure 3.2: Battery module In order to charge the batteries, we modified solar panel chargers: Figure 3.3: Simple battery charger In our experience, 5000mAh batteries lasted approximately 10 days (depending on the weather). Sticky Card Drawer Instead of sticking sticky cards directly on the lightbox, we strongly recommend using “drawers” on which you can pre-stick a card and swap drawers in and out. The printed drawers look like this: Figure 3.4: Sticky card drawer The part for the drawer is available on our Onshape repository. You can also just download the STL file. Note that you need a transparent material. You can also make a drawing from the Onshape part and laser-cut a of acrylic to do the job. Testing Station The testing station is used to adjust the focus of devices. It consists of two main parts: a testing tower, on which the “sledge” rests, and a perpendicular plane at the same distance as the sticky card would be: Figure 3.5: Sticky Pi testing platform To adjust the camera’s aperture, we can attach a lightbox in the back of the vertical wall ad manually turn it on with a modified USB cable. The precise dimensions of this platform are available on Onshape. "],["mobile-app.html", "Chapter 4 Mobile App Installation Configuration Use", " Chapter 4 Mobile App In order to retrieve the data from the Sticky Pis, we provide an Android mobile app Sticky Pi Data Harvester. This page describes how to install and use this app. Installation You can either install the app though the official Google play (WIP), or manually, using android studio (advanced). Through Google Play Work in progress Manual Installation This is the advanced approach, mostly for developers and testers. Clone the app repository Install Android Studio Make a new project for the app Compile and upload the app to the phone (using USB or wifi) Configuration In the “SETTINGS” menu, the API settings (Host, User name, Password) are the address (e.g. api.&lt;your-domain.com&gt; and logging credentials – with write access) for the harvester user. See the webserver documentation for detail. These settings are exclusively to upload data to the cloud. There are also user preferences you can tick: Delete local images after upload – to save storage on your phone, the app can delete images on the phone when they have been uploaded to the cloud (API) Enforce geolocation before syncing – this setting prevents you from retrieving data from Sticky Pis unless your GPS is functioning. The default (on) means Sticky Pis retrieve their approximate location from the phone when syncing. Use See the data harvsesting section of the user manual. "],["user-manual.html", "Chapter 5 User Manual First boot tunning and testing Maintenance and Data Harvesting", " Chapter 5 User Manual First boot tunning and testing As part of the assembly, it is crucial to initialise, tune and test each device. Here we show a video overview of the process, then we describe each step in more detail. Overview The main aims of this manual operation are: Perform the initial system boot Check system peripherals/sensors Calibrate the focus and aperture of the camera Pair devices to a data harvester Detail Prerequisites A testing platform A screen and an HDMI-HDMI-mini cable (or an adapter). You need a screen to adjust the focus dynamically A simple 5V power supply (splice an old USB cable and a female 2-pin auto-connector), or use a full battery A testing bridge (a two-pin female header on which the two pins connect). We advise to use a long cable to bridge the two pins. This way, you can attach the cable to the platform and make it impossible to “forget” the bridge on the board. A phone with the mobile app installed Before starting, in that order: Plug the screen (HDMI cable) into the Sticky Pi Bridge the testing pins (female bridge), see label on the Sticky Pi hat Power the board (female auto connector, 5V) As you plug in the board, you should immediately see the orange led indicator turning on, as in the video. If it does not turn on, unplug power at once and check your board for misplugged cables. Initial boot Important: do not unplug device during the first boot. When flashing our stock image file, the resulting operating system only occupies a small portion of the SD card (because we cannot know ahead how large the user’s SD card will be). During the very first boot, the system creates a new partition that fills all remaining available space, where the images will be saved. Furthermore, the system sets the RTC (hardware clock) to a dummy time (2000-01-01) as it cannot know the real time at this stage. Lastly, the system makes the OS partition read-only (so as not to corrupt itself during future operations). Throughout this process, the flash (from the lightbox) should blink, to indicate the process is still running. In the end, the system reboots itself. Testing When the system reboots it prints some debugging information on the screen. Pay special attention to: Device ID – a 8 hexadecimal digit you need to write down (it will come up again). Time – When testing a device for the first time, it should be set shortly after 2000-01-01. If not, check the coin cell on Sticky Pi Hat – if the cell is missing or drained, the clock will fail to keep time. Blinking flash – The lightbox should blink 10 times. If not, check the wiring of the light box, or retest the light box Reading DHT – should output sensible values of temperature and humidity. It may randomly fail at times – if after retesting several times DHT it keeps failing, check wiring and try swapping with another probe. Battery level – an internal analog sensor to read the voltage of the input power. When testing with USB (5V), the value should be near 1000. When testing with a lithium battery, it should be lower. Push button – the system waits for the push button to be activated. If you have pushed the button before (e.g. by accident), the system will say so and continue. If, whilst the system is waiting, you push the button, but nothing happens, check the button is properly wired. Camera testing and callibration – The system turns the camera on for a few seconds at a time. You can then visually assess focus. It is important that the testing rig places the camera at the same focal distance as it will be in the assembled device. We recommend putting a constantly powered (5V) lightbox back behind the imaging plane and working in a dark room to calibrate the aperture consistently. In our context, we calibrate the aperture so that images have an exposure time of ~30ms (this value will be printed on the screen between image previews), which works well during night and day, on yellow sticky cards. Draw or place objects on the imaging plane and adjust the focus until all are sharp and visible. When done, use a pair of pliers to lock the moving parts of the lens – and double check the image and exposure value. To move to the next step, remove the testing bridge. Pairing with data harvester – Start the data harvester app. Start a persistent wifi hotspot (as shown in video). Display the QR code for this hotspot, and place it in front of the camera. The system should detect the QR code and, after a few seconds, the device name/parameters will be shown on the phone screen. The device will remember this specific phone as long as the name/password of the hotspot remains unchanged. Note that it is possible to pair multiple phones to a single device, and this can be done after assembly/testing, when the devices are in the field. During pairing, the time of a device gets set to the time of the phone. After pairing, the screen attached to the Sticky Pi will display the time for the device and turn off. Note that the time is in UTC (not in your local timezone). Labeling – The Device ID displayed earlier is also the last device shown on the phone app. Write this number on the device (e.g. using a label maker). Always double check the ID and make multiple labels, as labels may peel off or become invisible in the field. We also like to glue an internal label on the back of the Pi sledge. Maintenance and Data Harvesting Maintenance Periodically, users should retrieve data and collect sticky cards. We suggest to do so every week. The video below shows how to perform the different steps: Labeling sticky cards Replacing batteries Harvesting data using the phone app Replacing the sticky cards Data Harvesting Process Ensure you have the mobile app installed. Then, Turn off the wifi on your phone Start a hotspot (see testing). Ideally, your Sticky Pi is paired to this phone/hotspot already. Otherwise you will need to pair using a QR code as described above. Push the power on button on a nearby Sticky Pi (note that several Sticky Pis) can be harvested in parallel After a few seconds, you will see devices appear on the “Devices” tab of the app. Pay attention to the fields in the device description (see figure below) Ensure all images have been downloaded (otherwise restart the Sticky Pi) "],["web-server.html", "Chapter 6 Web Server General description Deployment Additional information", " Chapter 6 Web Server This page describes how to deploy and maintain the web server. As much as we try to streamline this process, deploying our web server on the cloud does require some familiarity with Unix/Linux systems, Docker. We are open for collaboration and may be able to help you deploy your server, so do not hesitate to contact us. First, we describe the general function of the server. Second, we explain how to deploy and test it. Last, for completeness, we provide additional information on the individual components. General description Here is a schematics of how these services interact: Figure 6.1: Schematics of the web server services As shown in Figure 6.1, the web server is a suite of docker containers. The entry point is an Nginx server, spi_nginx, routes requests to the appropriate service: spi_api &lt;- api.* – our overall API server (i.e. a server that handles client requests to, for instance, upload/download images, create new users, retrieve image metadata, etc). spi_webapp &lt;- webapp.* – an Rshiny web interface to visualize data in real time. spi_s3 &lt;- s3.* – an optional local s3 server, using localstack. This is an alternative to subscribe to a commercial s3 server. The other services are not routed (i.e must not be accessible by external users): spi_db – a mariadb database that stores image metadata, user data, processing results, … spi_uid – a service that automatically pre-process all images, running the Universal Insect Detector, to segment insects vs background. certbot – a service based on certbot that issues and renews SSL certificates in the background (for HTTPS) Deployment Github repository The all the source code needed for deployment can be found on the sticky-pi-api github repository. It contains two important subdirectories: src, a python package used to create the API, and server, a set of docker services orchestrated with docker compose. For deployment, we only care about the server – if you are interested, the API is documented on readthedoc, but it should only concern you if you want to take part in development. Typically, you would clone this repository on your host. Hosting The simplest way to deploy the Sticky Pi server stack is on a remote Linux virtual machine. Perhaps your institution provides some cloud hosting, for instance Compute Canada Cloud, or you can opt for a commercial cloud provider. You can also decide to run the server on a custom machine within a restricted network. Security A remote web resources implies security considerations: Your host should use firewalls, the web-server only needs ports 80 (http) and 443 (https) open Ensure you use long, high entropy, passwords Update your host system Use ssh keys rather than passwords Configuration and important files Within the server directory, there are a few noticeable files we will describe: deploy.sh – A script you can run to deploy the whole stack. More details later. docker-compose.yml – The base docker-compose configuration file. docker-compose.prod.yml – The specification of the above file, for production. .env – An overall configuration file defining. You will need to modify some variables in it. .secret.env – Contains credentials. For security reasons, this file does not exist yet you need to create it. Do not share this file, and ensure the permissions are restrictive. .env file You need to create a directory that will contain all the server data (except the s3-hosted files). Set LOCAL_VOLUME_ROOT in the .env file accordingly: LOCAL_VOLUME_ROOT=/your/sticky-pi/root/dirctory. In order to encrypt data flow between client and server, we enforce https. That implies the registration of a domain name. Typically, you want to: Ensure your cloud provider can provide a static/floating IP address for your instance, say 111.112.113.114. Find a DNS provider and register your domain name, say sticky.net. Use your DNS provider to create two A records: sticky.net -&gt; 111.112.113.114 and *.sticky.net -&gt; 111.112.113.114 Define ROOT_DOMAIN_NAME in the .env file to your registered domain name. E.g. ROOT_DOMAIN_NAME=sticky.net .secret.env file Create or add to .secret.env file the variables by modifying this template: # En encryption key can be any long sting of ascii characters, # e.g. something like `cfbewif7y8rfy4w3aNIKFW9Yfh89HFN9` SECRET_API_KEY= # Your S3 host. We will describe further the difference between # external and local S3 provider # For local option we can set # S3_HOST=s3.${ROOT_DOMAIN_NAME} # for remote option this would be a hostname like `my-s3.provider.com` S3_HOST= # For local s3 option, you want to define arbitrary keys: # e.g. S3_ACCESS_KEY=fnwfnoAEVikenAV and # S3_PRIVATE_KEY=cfweavb87eabv8uehabv98hwAW7 # For remote option, you will need to issue a key pair and paste it S3_ACCESS_KEY= S3_PRIVATE_KEY= # An arbitrary password, only used internally # Should be more than 10-characters long MYSQL_PASSWORD= # Another arbitrary password, only used internally # Should be more than 10-characters long API_ADMIN_PASSWORD= # Your email address (or the main admin&#39;s) ADMIN_EMAIL= # The name of the S3 bucket storing the images # and other binary files. E.g. `sticky-pi-prod` S3_BUCKET_NAME= # An arbitrary password for the special user/internal bot `uid`, # which can preprocess images automatically (universal insect detector). # Should be more than 10-characters long UID_PASSWORD= Alternative local S3 server Our recommended option is to use an external provider for S3. However, you can also run your own S3 as a docker container. To do that you just need to set S3_HOST=s3.${ROOT_DOMAIN_NAME} in the .secret.env file. Your data will be stored, by default on ${LOCAL_VOLUME_ROOT}/s3. Beware that overall data can be large – typically [1, 2] GB per device per week. Deploy script Once we have set up the environment variables, we are ready to deploy. For convenience, we provide a script: server/deploy.sh that should be used this way for a production server: # sh deploy.sh prod-init # sh deploy.sh prod prod-init initializes SSL certificates. You should need to do that only once. If you want to subsequently restart/redeploy the server, just run sh deploy.sh prod. The first time, it may take a wile to build all the docker images. Once all the images have been built, ensure containers are running using docker ps. you can also debug and check the logs by using docker logs &lt;service_name&gt;. Check the API The simplest way to test the API end to end is to use cURL – you could also use our python client. For instance, we can ask the API to issue a temporary logging token for the admin user. Replace &lt;API_ADMIN_PASSWORD&gt; and &lt;ROOT_DOMAIN_NAME&gt; by their value (same as in .secret.env). curl --user admin:&lt;API_ADMIN_PASSWORD&gt; -X POST https://api.&lt;ROOT_DOMAIN_NAME&gt;/get_token This should return a json dictionary with the fields \"expiration\" and \"token\". If this does not work, stop here, and debug. Create users We can also use cURL to create users. For instance, to create a read only user named spi, with password R3ad_0nly, and no email address, we can run: curl --user admin:&lt;API_ADMIN_PASSWORD&gt; -X POST -H &quot;Content-Type: application/json&quot; -d &#39;[{&quot;username&quot;: &quot;spi&quot;, &quot;password&quot;: &quot;R3ad_0nly&quot;, &quot;is_admin&quot;: 0, &quot;email&quot;: &quot;&quot;, &quot;can_write&quot;:0}]&#39; https://api.&lt;ROOT_DOMAIN_NAME&gt;/put_users \"can_write\":1 would create a user that can write, and \"is_admin\": 1, an admin user (e.g. that can create other users). Note that you will not be able to retrieve a user’s password as passwords are internally encrypted. If a password is lost, the only way to log in is to reset it. You can list users with: curl --user admin:&lt;API_ADMIN_PASSWORD&gt; -X POST https://api.sticky-pi.com/get_users You can test users by requesting a token, using curl as above, replacing --user admin:&lt;API_ADMIN_PASSWORD&gt;, by --user &lt;USERNAME&gt;:&lt;USER_PASSWORD&gt;. Backups The raw image data is on the s3 server. Manual backups/archives can be done by downlaoding all data to a separate resource/hard drive/… with tools such as s3cmd, using your credentials. In order to backup mariadb, you can use the deploy script: #sh deploy.sh mysql-backup This will create a file like spi_db.dump.YYYY-MM-DD.sql.gz in the current working directory – that you then manually copy/archive. In case of catastrofic failure, the database could be restored with it. Additional information Database The database (spi_db) is the stock MariaDB. The database data is mounted on a persistent directory on the host: ${LOCAL_VOLUME_ROOT}/mysql in the host maps /var/lib/mysql in the container. Nginx Nginx (spi_nginx) is the stock Nginx. We use a custom configuration file to define routing (nginx/nging.conf-template). Note that this is a template file where we inject environment variable at built time. You should not need to modify the configuration file for standard deployment API The API (spi_api) is a uwsgi + flask server. The main file is api/app.py. This server depends extensively on our python package, sticky_pi_api. Note that the API and client are packaged together, which allows us to use mock local API vs remote API. Universal Insect Detector The UID (spi_uid) is a simple python script (ml/uid.py) that uses the API client and the our sticky_pi_ml package. It fetches the images that are not annotated (or obsolete) from client, analyse them, and upload the results (to the database, through the API). This process runs automatically. Note that you need to have either trained the UID or download a reference model for this service to work. We explain how to set-up ML resources in a dedicated section. Webapp The Webapp (spi_webapp) is an Rshiny interactive website. It is mainly designed to visualise image and processing data. Typically hundreds of images are acquired every week, this interface allows you to select a date range to display a plot of environmental conditions and overlay images as well as UID results. This is very helpful to ensure the quality of your data in real time. The webapp will be available at webapp.&lt;ROOT_DOMAIN_NAME&gt;, as defined in hosting. "],["ml.html", "Chapter 7 Machine Learning General description General Prerequisites Universal Insect Detector Siamese Insect Matcher Insect Tuboid Classifier Conclusion", " Chapter 7 Machine Learning This page describes how to annotate, train and use the machine learning model of the Sticky Pi project. This step does require some familiarity with scientific computing (e.g. Unix/Linux, python. Here, we complement the descriptions of the algorithms provided in the Sticky Pi manuscript with a practical documentation on how to use and adapt them. We are open for collaboration regarding training, building and extending Machine Learning models, so do not hesitate to contact us. For simplicity, we describe how to use the ML tools independently of the API (which is set to store and query images/annotations on a remote or a local machine). For production, with multiple concurrent Sticky Pi devices, we recommend using the API. General description Briefly, we start from a series of images (every 20min, for several days) of a sticky card. Our ultimate goal is to tell which, and when insects were captured. We break down this task in three independent steps: Universal Insect Detector – We perform an instance segmentation on all images independently. This extracts insects (foreground) from the background. Importantly, at this stage, we do not yet classify insects. Siamese Insect Matcher – Captured insects actually may move, degrade, become occluded… Therefore, in practice, classification and timing of capture from single images would be very inconsistent through time. Instead, we first track insects before classifying them. The function of the Siamese Insect Matcher is to track multiple insect instances through their respective timelapses. Insect Tuboid Classifier – After tracking, each insect in the time series is represented by a variable number of standardized segmented shots as well as metadata (including size) – which we call a “tuboid”. The Insect Tuboid Classifier infers a taxonomy to each tuboid based on multiple images. Below, we describe how to implement each step. The model files and datasets used in the publication are available on Zenodo. Our source code is publicly available on github. General Prerequisites Installation We recommend starting by setting a Python virtual environment for the entire project. And using the python package manager (pip). Detectron2, PyTorch and PyTorch Vision In your virtual environment, you want to manually install detectron2, which requires matching PyTorch and Torchvision. This has to be done manually since it depends on your platform/hardware (e.g. GPU support). For instance, on a Linux machine without CUDA (so, no GPU support), from the pytorch website: # installing precompiled PyTorch and Torchvision (from at https://pytorch.org/) pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu # then building detectron2 from source # you could also use prebuilt versions pip3 install &#39;git+https://github.com/facebookresearch/detectron2.git&#39; Note, to make the most of CNNs and PyTorch, you likely want to have hardware support (i.e. a GPU). Running models on a CPU is mostly for testing and development, and may be very slow (in particular for training). sticky-pi-ml Once you have installed detectron2 (along with PyTorch and PyTorch Vision), you can install our package and its dependencies. git clone https://github.com/sticky-pi/sticky-pi-ml.git --depth=1 cd sticky-pi-ml/src pip install . Project organisation In the Sticky Pi project, the resources for each of the three algorithms described above (i.e. UID, SIM and ITC) are stored and organised in a “Machine Learning (ML) bundle”. An ML bundle is a directory that contains everything needed to train, validate and use a tool. ML bundles all contain the subdirectories: config – one or several .yaml configuration files data – the training and validation data output – the trained model (i.e. .pth files). The file model_final.pth being the working model used for inference. Universal Insect Detector The goal of (Universal Insect Detector) UID is to find and segment all individual insects from arbitrary sticky card images. As part of the sticky-pi-ml package, we have made a standalone tool version of the UID, standalone_uid.py (see standalone_uid.py --help). From within your virtual environment, you can use this tool (it should be in your path after installation) to segment images as well as re-train and validate the model on your own data. We can start by downloading the whole ML Bundle from Zenodo (that can take a while as we are getting both the model and the data): wget https://zenodo.org/record/6382496/files/universal-insect-detector.zip unzip universal-insect-detector.zip # show what is inside this new directory ls universal-insect-detector Our bundle directory is therefore universal-insect-detector. Inference For inference with the standalone tool, all images should be .jpg stored in a directory structure. For this example, you could download a sample of three images we have put together for this tutorial. wget https://doc.sticky-pi.com/assets/uid_tutorial.zip unzip uid_tutorial.zip ls uid_tutorial Or just use the download link Then, we can use our custom script standalone_uid.py to “predict” – i.e. segment the images: # We use the uid tool to predict, based on a model located in the bundle `./universal-insect-detector` (--bundle-dir) # We find all `.jpg` files in the target directory `./uid_tutorial` (--target) # We set the verbose on (-v) standalone_uid.py predict_dir --bundle-dir ./universal-insect-detector \\ --target ./uid_tutorial -v # list the generated resulting files ls ./uid_tutorial/*.svg As a result, the uid tool makes an SVG image for each JPG. The SVG contains a path for each detected instance. You can directly open the SVG files to check. By default, the inference tool does not overwrite existing SVG, unless you use the --force flag. Training Data source In order to train the model, you need to populate the universal-insect-detector/data/. For the UID, the input files are SVGs exactly like the ones outputted by the inference tool (i.e. a JPG image is embedded and each insect is a path). You can either add new data to the existing collection of SVGs already present in universal-insect-detector/data, or rebuild your own new set (though the later option would be rather labour-intensive). A simple way to get started, is to run inference on your new images (as describe just above), and fix/check the resulting SVGs by hand. Alternatively, you use the --de-novo option along with --predict-dir to just wrap your images in an empty SVG. To edit the SVG, we recommend using inkscape. The default is that every insect is a simple, closed, path/polygon (e.g. made with the Bezier curve tool). The stroke colour of the path defines the class of the object (the filling colour does not matter). The default stroke colour for insects is in blue #0000FF (This is defined in the configuration file. Any other colour will not be recognised as an insect): UID annotations. Using inkscape to generate annotations as SVG paths Configuration There are two configuration files for the UID: mask_rcnn_R_101_C4_3x.yaml – the configuration for Mask-RCNN as defined in the detectron2 documentation. This is the underlying segmentation algorithm we use. config.yaml – The specific configuration for the Sticky Pi project (which may override some of the mask_rcnn_R_101_C4_3x.yaml configuration). See inline comments for detail. The important configuration variables are likely going to be in the SOLVER section: IMS_PER_BATCH – the number of images in a training bash. The larger this number, the more memory will be used during training. This will depend on your GPU capabilities. BASE_LR – The starting learning rate GAMMA – The decay rate of the learning rate, at every ‘step’ STEPS– The training steps, in number of iterations. at each step, the learning rate will decrease (by a factor GAMMA) Data integrity Before training, you most likely want to check your data can indeed be loaded and parsed. standalone_uid.py check_data --bundle-dir ./universal-insect-detector -v Carefully read the warnings, in particular if they hint that the SVG paths are malformed. Training The training itself can be very long (e.g. several days on a GPU can be expected). Likely, you have access to specialised hardware and support to do that. Once you have set the configuration, checked the input data, etc, you can use the standalone UID tool to make a new model. standalone_uid.py train --bundle-dir ./universal-insect-detector -v The script will output information about the dataset and print a summary every 20 iterations (by default). Each summary contains information such as total_loss, which should eventually decrease (this is described in the detectron2 documentation). Every 5000 iterations (defined in the configuration as SOLVER/CHECKPOINT_PERIOD), a snapshot of the ongoing model will be generated as universal-insect-detector/output/model_XXXXXX.pth, where XXXXXX is the iteration number. Unless you have reached the maximal number of iterations, you will need to manually copy your latest snapshot into the final working model universal-insect-detector/output/model_final.pth. You could use the intermediary snapshots to perform custom validation, or eventually remove them. If you want to train the model from “scratch”, use the --restart_training flag. This will actually use Mask-RCNN model that was pretrained on the COCO dataset (training from zero would take much longer). Validation An important step is validation. By default, each original image is allocated to either a validation (25%) or training (75%) dataset. This is based on the checksum of the JPG image, so it is pseudorandom. To compute validation statistics on 25% of the images that were “randomly” excluded from training, you can run: standalone_uid.py validate --bundle-dir ./universal-insect-detector \\ --target validation_results -v This will run an inference on the validation set which has not been seen during training, create a resulting SVG files and issue summary statistics for each validation image (all in the target directory validation_results/). In particular, there will be a resulting JSON file results.json, which contains a list where each element is a detected instance. Each instance has the fields: area – the number of pixels in the instance in_gt – whether the instance is in the ground truth in_im – whether the instance is in the detected image class – the class (i.e. insect) filename – the SVG file where the image is from You can then parse this file (e.g. in R) to compute summary statistics: library(data.table) library(jsonlite) dt &lt;- as.data.table(jsonlite::fromJSON(&#39;results.json&#39;)) dt[, .(precision = sum(in_gt &amp; in_im)/ sum(in_im), recall = sum(in_gt &amp; in_im)/ sum(in_gt)), ] You can also compare the validation images (SVG) generated in the target directory (validation_results/). Each path is a detected instance. The ones filled with red (#ff0000) are detected by the UID, whilst the blue (#0000ff) ones are the ground truth. I find it convenient to open images in inkscape and select, say a UID-generated path, right click and press “select same/fill and stroke” to then change the stroke style and colour to visualise better: Validation results. Left, the original svg; Right, highlighting the ground truth in thick green strokes Siamese Insect Matcher The Siamese Insect Matcher is the second step of the analysis. We start from a series of images annotated by the UID to generate “tuboids”, which are series of shots of the same insect, over time. To do that, we use annotated Sticky Pi images. In the previous section, we described how to generate SVG images from JPGs in a directory. The SIM is specific to Sticky Pi images as their timestamp is encoded in their name. The name of each image is formatted as &lt;device_name&gt;.&lt;datetime&gt;.jpg. We also have a standalone tool to use the SIM standalone_sim.py (see standalone_sim.py --help). We can start by downloading the whole ML Bundle from Zenodo (that can take a while as we are getting both the model and the data): wget https://zenodo.org/record/6382496/files/siamese-insect-matcher.zip unzip siamese-insect-matcher.zip # show what is inside this new directory ls siamese-insect-matcher Inference For this documentation, we have made available a small series of 50 pre-annotated images. First we download the images: wget https://doc.sticky-pi.com/assets/sim_tutorial.zip unzip sim_tutorial.zip ls sim_tutorial Or just use the download link. Using the standalone tool, you can then generate tuboids: standalone_sim.py predict_dir --bundle-dir ./siamese-insect-matcher \\ --target ./sim_tutorial -v This should show progress and processing information. As a result, you will find a directory named tuboids in --target (i.e. ./sim_tutorial), with the following structure: tuboids/ └── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f ├── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f.0000 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg ├── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f.0001 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg ..................... SKIPPING DIRECTORY WITH SIMILAR STRUCTURE ..................... ├── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f.0019 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg └── 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.mp4 The parent directory, 15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.1611328841-264f9489eac5c0966ef34ff59998084f, is formatted as: &lt;device_id&gt;.&lt;start_datetime&gt;.&lt;end-datetime&gt;.&lt;algorithm_unique_id&gt;. The children directories are unique identifiers of each tuboid in this specific series (ending in .0000, .0001, .0002, …). Inside each tuboid directory, there are three files: metadata.txt – a small file describing each insect (parent image, x and y position, and scale) tuboid.jpg – a JPG file where shots listed in metadata are represented as contiguous tiles. The file is padded with empty space when necessary context.jpg – a compressed version of the first image containing the insect, where the insect is boxed (to show where and how large it is). This is mostly to give some context to the annotating team. You will also find a video (15e612cd.2020-07-08_22-05-10.2020-07-09_13-27-20.mp4): Note that each tuboid is shown by a rectangle with a number matching the unique identifier of a tuboid. This video can help you find a specific tuboid in the tuboids directory structure. For instance, tuboid number 12 looks like this: Also, the UTC date and time are written on top of the video. Training The core algorithm of the Siamese Insect Matcher is a Siamese neural network. It compares insects from consecutive pictures and learns to discriminate between the same instance vs. another insect. The data to train this network is a set of paired UID-annotated images, where insects that are the same instances are labeled as a group. In practice, these are encoded as composite SVG images with the two images vertically stacked, and the instances labeled as an SVG group. Rather than manually generating a training set, we can use the standalone tool to generate candidates for us that we can then just amend: standalone_sim.py make_candidates --filter 5 --target ./sim_tutorial -v The argument --filter 5 keep only every fifth image, and skip the others. By default, this process will pre-group pairs of contours that highly overlap between consecutive images (based on their IoU only). This should save a lot of time as most of these should be simple cases. This tool also draws a line to highlight paired groups. The line is simply a visual tool to help annotation and can be left as is: Generate SIM annotations in Inkscape Using inkscape, you can group pairs using a shortcut like CTRL+G. We recommend using the XML editor (the panel on the right) to check that the grouping is correct, and find unmatched instances. Keep in mind that not all instances can / should be matched. Indeed, some insects may be mislabeled in the first place, or may have escaped, appeared, … It is unnecessary (and probably counterproductive) to try to manually fix the segmentation (i.e. the UID) by adding or deleting new instance annotations (paths). As before, you can either make your own data set, or just extend the existing SIM data bundle (the latter is recommended). For the training itself, we can run: standalone_sim.py train --bundle-dir ./siamese-insect-matcher -v On a machine with CUDA, you should use the --gpu flag. Also you can use the --restart-training flag to restart from scratch rather than use the previous weights. The training involves two preliminary stages that independently train separate branches of the network followed by a longer third (final) stage that trains the whole network. Every 200 rounds (default), the model takes a snapshot (i.e. saves the weights) in siamese-insect-matcher/output/model_XXXXXX.pth. You can also manually the any .pth file as model_final.pth to specifically use this model during inference Validation Like for the UID, each original image is allocated to either a validation (25%) or a training (75%) dataset based on a checksum. Validation is automatically computed to generate a loss and accuracy when the training reaches a checkpoint (by default, every 200 rounds). One can also specifically run validation: standalone_sim.py validate --bundle-dir ./siamese-insect-matcher \\ --target ./sim_tutorial/validation/ -v This will generate a json file ./sim_tutorial/validation/results.json, which is a list of dictionaries with fields pred (predicted score) and gt (ground truth, i.e. 0 or 1). pred is the raw match score (between 0 and 1), so one can use this result file to vary the match threshold and make an ROC curve. E.g. to make a ROC curve in R: library(plotROC) library(data.table) library(jsonlite) dt &lt;- as.data.table(jsonlite::fromJSON(&#39;results.json&#39;)) rocplot &lt;- ggplot(dt, aes(m = pred, d = gt))+ geom_roc(n.cuts=20,labels=FALSE) rocplot + style_roc(theme = theme_grey) + geom_rocci(fill=&quot;pink&quot;) Insect Tuboid Classifier The Insect Tuboid Classifier (ITC) is the last step of the analysis. Its goal is to infer taxonomy from tuboids (several shot of an insect). It is an adapted ResNet architecture that: takes multiple images as input, averages them on the feature space, and outputs one prediction. This algorithm also uses the initial pixel-size of the insect, which is contained in the tuboid metadata. As opposed the other two generalist algorithms, the ITC will need to be retrained for specific contexts (location, season, …). Therefore, we will first describe how to train the algorithm. Training Ground-truth data The previous algorithm (SIM) generated “tuboids”, each of which corresponds to a unique insect. These are uniquely identified directories. The goal of the annotation is to associate a taxonomy to a set of reference tuboids. In order to facilitate annotation of many tuboid, we have made an open source multi-user web tool. Taxonomy is the hardest task, so multiple experts might be involved. In order to use the annotation tool, two steps are required: Set up an S3 bucket that contains the tuboid data. Configure and deploy our docker service Upload data on an S3 server S3 servers are a standard cloud solution for hosting and serving large amounts of data on the cloud. There are different providers and tools to interface S3 server. Here, we show how to do that with s3cmd. First, we make a new bucket: S3_BUCKET=&quot;a-sensible-name&quot; s3cmd mb s3://${S3_BUCKET} Then, we upload the local tuboid data. We assume, you have a directory ($TUBOID_DATA_DIR) with the tuboid data. This would be typically a subdirectory named tuboids in the --target directory you set when using the SIM for inference. To practice, you can use tutorial data we have put together for the ITC: wget https://doc.sticky-pi.com/assets/itc_tutorial.zip unzip itc_tutorial.zip ls itc_tutorial Or just use the download link TUBOID_DATA_DIR=itc_tutorial/data # or REPLACE_WITH_YOUR_PATH # this send the local data on the remote s3 server s3cmd sync ${TUBOID_DATA_DIR}/ s3://${S3_BUCKET} Obviously, check that files are being uploaded. We also pre-generate an index file that list all the tuboids. TMP_DIR=$(mktemp -d) echo &#39;tuboid_id, tuboid_dir&#39; &gt; ${TMP_DIR}/index.csv # this loop generates the index file for i in $( s3cmd ls s3://${S3_BUCKET}/ --recursive | \\ grep metadata\\.txt | \\ cut -d : -f 3 | \\ cut -c2-); do echo $(basename $(dirname $i)), $(dirname $(realpath --relative-to=&quot;/${S3_BUCKET}&quot; $i -m)) \\ &gt;&gt; ${TMP_DIR}/index.csv; done # for each tuboid, 3 files are uploaded (`metadata.txt`, `tuboid.jpg` and `context.jpg`) head ${TMP_DIR}/index.csv # just to take a look at the index file wc -l ${TMP_DIR}/index.csv # the number of line should be the number of tuboids +1 (for the header) s3cmd put ${TMP_DIR}/index.csv s3://${S3_BUCKET} &amp;&amp; rm ${TMP_DIR} -r On your s3 service provider, generate an access key specifically for the docker service. Ideally, the access key pair is read only and only has access to the bucket you just created. Do not share the private key, but keep it as you will need it to configure the docker service. Set up the docker service Docker is an industry standard container service. It allows you to run a virtualised self-contained service on top of a host machine. You can set docker on your machine, a local or a remote server. There are also a range of commercial docker hosting platform. To run our docker container, first create a local environment file to store your variables, including s3 credentials, for instance .secret.env (ensure this file has the appropriate access set; i.e. only you/ the administrator should be able to read this file). In .secret.env, we can edit the environment variables according to the comment: export S3_HOST=some.host.com # The host of the bucket e.g. `location. export S3_BUCKET=my-bucket-name # the name of the s3 bucket. ame as above export S3_PRIVATE_KEY=abcdefgh # The private key for this service (see above) export S3_ACCESS_KEY=ABCDEF # The access key for this service (see above) export PORT=80 # The network port on the host on which the service will be available (80 for http) export LOCAL_VOLUME=/my/local/storage # A local dirctory on the host that contain the service&#39;s data (e.g. the results) Then to run the container, you can do: source ./.secret.env docker run --rm --publish ${PORT}:80 \\ --name tuboid-annotation-tool \\ --env S3_BUCKET=${S3_BUCKET} \\ --env S3_HOST=${S3_HOST} \\ --env S3_ACCESS_KEY=${S3_ACCESS_KEY} \\ --env S3_PRIVATE_KEY=${S3_PRIVATE_KEY} \\ --volume ${LOCAL_VOLUME}:/opt/data_root_dir \\ -d stickypi/tuboid-annotation-tool:latest The first time, this will download the image from the dockerhub, where we have stored a copy of the prebuilt docker image. For this simple application, we just use plaintext credentials as a json file, credentials.json, in LOCAL_VOLUME. For instance, in ${LOCAL_VOLUME}/credentials.json, yo0u could have: { &quot;user1&quot; : {&quot;password&quot;: &quot;password1&quot;, &quot;allow_write&quot;: 1}, &quot;user2&quot; : {&quot;password&quot;: &quot;password2&quot;, &quot;allow_write&quot;: 0} } This creates two users, user1 and user2, with passwords, password1 and password2, respectively. allow_write=1 means the user (user1) has write access. user2 can only view images, but cannot annotate. If you run the annotation tool on your local computer, you should be able to access it on http://localhost:&lt;PORT&gt;, where &lt;PORT&gt; is the port you defined in your configuration file. This video shows how you can use the tool to annotate tuboid images and export the data: Training the algorithm Once you have annotated your data, you can retrieve database.db file from the annotation web tool. You will need to put this file in the data subdirectory of your bundle directory. If you have used the tutorial bundle, it should look like that: ├── config │   └── config.yaml └── data ├── database.db ├── 0a5bb6f4.2020-07-08_22-00-00.2020-07-15_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694 │   └── 0a5bb6f4.2020-07-08_22-00-00.2020-07-15_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694.0000 │   ├── context.jpg │   ├── metadata.txt │   └── tuboid.jpg ├── 15e612cd.2020-07-01_22-00-00.2020-07-08_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694 │   ├── 15e612cd.2020-07-01_22-00-00.2020-07-08_12-00-00.1607154774-d74d75f50086077dbab6b1dce8c02694.0000 │   │   ├── context.jpg │   │   ├── metadata.txt │   │   └── tuboid.jpg ..................... SKIPPING DIRECTORY WITH SIMILAR STRUCTURE ..................... As for the two other tools, we have built a standalone executable for the ITC. Given itc_tutorial/ is your bundle dir, you can then train the model using: standalone_itc.py train --bundle-dir itc_tutorial/ This will start training the model from scratch and output validation statistics and confusion matrices at every checkpoint. As a functional example, you can download and train the model we used for the manuscript: We can start by downloading the whole ML Bundle from Zenodo (that can take a while as we are getting both the model and the data): wget https://zenodo.org/record/6382496/files/insect-tuboid-classifier.zip unzip insect-tuboid-classifier.zip # show what is inside this new directory ls insect-tuboid-classifier Importantly, you need to adapt the configuration file config.yaml to your scenario. In particular, in this model, we define flat labels from taxonomy, using regular expressions. The taxonomical levels are: type, order,family,genus,species, and extra (extra could be other information such as sex and morph). Labels are defined as [regex, number], in config.yaml: LABELS: - [&#39;^Background.*&#39;,0] - [&#39;^Insecta\\.Hemiptera\\.Cicadellidae\\.Edwardsiana.*&#39;, null] - [&#39;^Insecta\\.Hemiptera\\.Cicadellidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Drosophilidae\\.Drosophila\\.Drosophila suzukii.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Drosophilidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Psychodidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Culicidae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Muscidae.*&#39;,null] - [&#39;^Insecta\\.Diptera\\.Sciaridae.*&#39;, null] - [&#39;^Insecta\\.Diptera\\.Syrphidae.*&#39;, null] - [&#39;^Insecta\\.Coleoptera\\.Curculionidae.*&#39;,null] - [&#39;^Insecta\\.Coleoptera\\.Coccinellidae.*&#39;,null] - [&#39;^Insecta\\.Coleoptera\\.Elateridae.*&#39;,null] - [&#39;^Insecta\\.Coleoptera.*&#39;,null] - [&#39;^Insecta\\.Hymenoptera\\.Figitidae.*&#39;, null] - [&#39;^Insecta\\.Hymenoptera\\.Halictidae.*&#39;,null] - [&#39;^Insecta\\.Lepidoptera.*&#39;, null] - [&#39;^Insecta.*&#39;,1] regex is a regular expression matching the taxonomy. All taxonomic levels are collapsed in a single sting separated by .. For instance, Drosophila suzukii is defined as ^Insecta\\.Diptera\\.Drosophilidae\\.Drosophila\\.Drosophila suzukii.* number is an integer &gt;=0, for this label. If null, the labels automatically take the next available value. (here we set background and undefined insects as 0 and 1, respectively). Since multiple labels may match an annotation, the order of definition of labels matter. A given annotation will take the value of the first matched label. For instance, if we have an annotation that is Insecta\\.Coleoptera\\.Elateridae.abcd.efg, it matches both ^Insecta\\.Coleoptera\\.Elateridae.* and ^Insecta\\.Coleoptera.*. Since ^Insecta\\.Coleoptera\\.Elateridae.*, comes first, we keep it as a label. Inference Since we already have a functional model for the manuscript data, we can use it to demonstrate how the inference works. Given you store the ML bundle for the ITC in insect-tuboid-classifier/ we could run inference, using this model, on its own data --target insect-tuboid-classifier/data. standalone_itc.py predict_dir --bundle-dir insect-tuboid-classifier --target insect-tuboid-classifier/data -v This will generate a CSV file, insect-tuboid-classifier/data/results.csv\" with column names describing the taxonomy and the name/origin of unique tuboids in the column directory. Conclusion We have covered the basics of how to perform machine learning with the Sticky Pi project. Hopefully this is a good starting point to use and adapt our tools. The project is still active, and we are always welcoming contributions and collaborations. "],["outreach.html", "Chapter 8 Outreach UBC Campus Living Lab June-August 2021", " Chapter 8 Outreach Sticky Pis are built on top of community hardware such as Arduino and Raspberry Pi as well as open-source software such as Python. We share their educational spirit and consider it our work to raise awareness about insect ecology and discuss how new technologies can indeed intersect with the study of insects. UBC Campus Living Lab In 2022, we were awarded the Campus Living Lab fund by UBC sustainability. The CLL will allow us to improve and deploy Sticky Pis all over the UBC campus to monitor biodiversity in real-time. We will also involve the community by running workshops and hackathons. June-August 2021 Figure 8.1: Sticky Pi at the UBC Research Farm In 2021, we were able to deploy 36 Sticky Pi traps at the University of British Columbia farm, a research, teaching and learning space, located on the traditional, ancestral, and unceded territory of the hən̓q̓əmin̓əm̓-speaking xʷməθkʷəy̓əm (Musqueam) people, and housed under the Faculty of Land and Food Systems. Our goal was to monitor insect populations in an organic strawberry field interspersed with different flowers (aka intercropping). Neighbouring flowers are known to attract pollinators and other beneficial insects that can then help to control pests. We wanted to study how different flowers such as buckwheat, clover and dill indirectly mitigate the infestation by the spotted wing drosophila (a voracious fruit pest). Intercropping could be a tool within a suite of integrated solutions to maintain production whilst reducing the environmental impact of agriculture. The experiment is ongoing and we will communicate the results later this year. "],["community.html", "Chapter 9 Issues and community Citation Having troubles Contributing Authors and Contacts", " Chapter 9 Issues and community Citation Sticky Pis are a research tool. As such, they are – and will always be – as free and open-source as possible. If you use Sticky Pis as part of your research, please cite our reference publication. Having troubles If you are having issues or you want help with something, the best thing you can do is fill an “issue” on the github repository of the relevant tool. Contributing We welcome external contributions and hope Sticky Pis develops as part of a computational entomology community. There are several ways you can contribute: By requesting features and reporting bugs through the github issue system By sending pull requests to preexisting tools By volunteering to maintain/develop a tools Authors and Contacts Sticky Pis are developed at the Plant-Insect Ecology and Evolution lab at UBC, Vancouver in collaboration with the Haney Lab and the Insect Biocontrol lab. Do not hesitate to contact Quentin Geissmann, the main contributor of the project, for inquiries. Daphne Chevalier and Wei (Tom) Xie helped generating and curating content for this documentation. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
